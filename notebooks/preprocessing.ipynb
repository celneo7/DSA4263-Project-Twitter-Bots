{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69dd6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from dotenv import load_dotenv\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15325fc5",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a7abf3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>profile_background_image_url</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>average_tweets_per_day</th>\n",
       "      <th>account_age_days</th>\n",
       "      <th>account_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-15 21:32:11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Blame @xaiax, Inspired by @MakingInvisible, us...</td>\n",
       "      <td>4</td>\n",
       "      <td>1589</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>787405734442958848</td>\n",
       "      <td>en</td>\n",
       "      <td>unknown</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7874121826...</td>\n",
       "      <td>best_in_dumbest</td>\n",
       "      <td>11041</td>\n",
       "      <td>False</td>\n",
       "      <td>7.870</td>\n",
       "      <td>1403</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-09 05:01:30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Photographing the American West since 1980. I ...</td>\n",
       "      <td>536</td>\n",
       "      <td>860</td>\n",
       "      <td>880</td>\n",
       "      <td>False</td>\n",
       "      <td>796216118331310080</td>\n",
       "      <td>en</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8023296328...</td>\n",
       "      <td>CJRubinPhoto</td>\n",
       "      <td>252</td>\n",
       "      <td>False</td>\n",
       "      <td>0.183</td>\n",
       "      <td>1379</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-17 05:34:27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Scruffy looking nerf herder and @twitch broadc...</td>\n",
       "      <td>3307</td>\n",
       "      <td>172</td>\n",
       "      <td>594</td>\n",
       "      <td>True</td>\n",
       "      <td>875949740503859204</td>\n",
       "      <td>en</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1278890453...</td>\n",
       "      <td>SVGEGENT</td>\n",
       "      <td>1001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1159</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-21 13:32:25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Wife.Godmother.Friend.Feline Fanatic! Assistan...</td>\n",
       "      <td>8433</td>\n",
       "      <td>517</td>\n",
       "      <td>633</td>\n",
       "      <td>True</td>\n",
       "      <td>756119643622735875</td>\n",
       "      <td>en</td>\n",
       "      <td>Birmingham, AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1284884924...</td>\n",
       "      <td>TinkerVHELPK5</td>\n",
       "      <td>1324</td>\n",
       "      <td>False</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1489</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-15 16:32:35</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Loan coach at @mancity &amp; Aspiring DJ</td>\n",
       "      <td>88</td>\n",
       "      <td>753678</td>\n",
       "      <td>116</td>\n",
       "      <td>True</td>\n",
       "      <td>464781334</td>\n",
       "      <td>en</td>\n",
       "      <td>England, United Kingdom</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/9952566258...</td>\n",
       "      <td>JoleonLescott</td>\n",
       "      <td>4202</td>\n",
       "      <td>True</td>\n",
       "      <td>1.339</td>\n",
       "      <td>3138</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at  default_profile  default_profile_image  \\\n",
       "0  2016-10-15 21:32:11            False                  False   \n",
       "1  2016-11-09 05:01:30            False                  False   \n",
       "2  2017-06-17 05:34:27            False                  False   \n",
       "3  2016-07-21 13:32:25             True                  False   \n",
       "4  2012-01-15 16:32:35            False                  False   \n",
       "\n",
       "                                         description  favourites_count  \\\n",
       "0  Blame @xaiax, Inspired by @MakingInvisible, us...                 4   \n",
       "1  Photographing the American West since 1980. I ...               536   \n",
       "2  Scruffy looking nerf herder and @twitch broadc...              3307   \n",
       "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...              8433   \n",
       "4               Loan coach at @mancity & Aspiring DJ                88   \n",
       "\n",
       "   followers_count  friends_count  geo_enabled                  id lang  \\\n",
       "0             1589              4        False  787405734442958848   en   \n",
       "1              860            880        False  796216118331310080   en   \n",
       "2              172            594         True  875949740503859204   en   \n",
       "3              517            633         True  756119643622735875   en   \n",
       "4           753678            116         True           464781334   en   \n",
       "\n",
       "                  location                      profile_background_image_url  \\\n",
       "0                  unknown  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "1           Estados Unidos  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "2          Los Angeles, CA  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "3           Birmingham, AL                                               NaN   \n",
       "4  England, United Kingdom  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "\n",
       "                                   profile_image_url      screen_name  \\\n",
       "0  http://pbs.twimg.com/profile_images/7874121826...  best_in_dumbest   \n",
       "1  http://pbs.twimg.com/profile_images/8023296328...     CJRubinPhoto   \n",
       "2  http://pbs.twimg.com/profile_images/1278890453...         SVGEGENT   \n",
       "3  http://pbs.twimg.com/profile_images/1284884924...    TinkerVHELPK5   \n",
       "4  http://pbs.twimg.com/profile_images/9952566258...    JoleonLescott   \n",
       "\n",
       "   statuses_count  verified  average_tweets_per_day  account_age_days  \\\n",
       "0           11041     False                   7.870              1403   \n",
       "1             252     False                   0.183              1379   \n",
       "2            1001     False                   0.864              1159   \n",
       "3            1324     False                   0.889              1489   \n",
       "4            4202      True                   1.339              3138   \n",
       "\n",
       "  account_type  \n",
       "0          bot  \n",
       "1        human  \n",
       "2        human  \n",
       "3        human  \n",
       "4        human  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id = \"1WPXfR8a8gmiShFoNbGyY9NFfMpQ1A61a\"\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "raw_data = pd.read_csv(download_url, index_col = 0)\n",
    "df = raw_data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f843d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/raw/raw_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c76f62",
   "metadata": {},
   "source": [
    "# Creation of New Features for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1d9aaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\CelesteN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "### functions to extract features of descriptions\n",
    "def safe_word_count(desc):\n",
    "    if pd.isna(desc) or not isinstance(desc, str):\n",
    "        return 0\n",
    "    return len(desc.split())\n",
    "\n",
    "\n",
    "def safe_mean_word_length(desc):\n",
    "    if pd.isna(desc) or not isinstance(desc, str):\n",
    "        return 0\n",
    "    words = desc.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return np.mean([len(word) for word in words])\n",
    "\n",
    "\n",
    "def safe_mean_sent_length(desc):\n",
    "    if pd.isna(desc) or not isinstance(desc, str):\n",
    "        return 0\n",
    "    sents = sent_tokenize(desc)\n",
    "    if len(sents) == 0:\n",
    "        return 0\n",
    "    return np.mean([len(sent) for sent in sents])\n",
    "\n",
    "\n",
    "def count_hashtags(desc):\n",
    "    if isinstance(desc, str):\n",
    "        return desc.count(\"#\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def count_handles(desc):\n",
    "    if isinstance(desc, str):\n",
    "        return desc.count(\"@\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def count_urls(desc):\n",
    "    pattern = r\"htt[s?://\\S+]\"\n",
    "    if isinstance(desc, str):\n",
    "        matches = re.findall(pattern, desc)\n",
    "        return len(matches)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"] = df[\"description\"].str.len().fillna(0)\n",
    "df[\"word_count\"] = df[\"description\"].apply(safe_word_count)\n",
    "df[\"mean_word_length\"] = df[\"description\"].apply(safe_mean_word_length)\n",
    "df[\"mean_sent_length\"] = df[\"description\"].apply(safe_mean_sent_length)\n",
    "df[\"hashtag_count\"] = df[\"description\"].apply(count_hashtags)\n",
    "df[\"handle_count\"] = df[\"description\"].apply(count_handles)\n",
    "df['url_count'] = df[\"description\"].apply(count_urls)\n",
    "\n",
    "# save to csv\n",
    "df[['id', 'length', 'word_count', 'mean_word_length', 'mean_sent_length', 'hashtag_count', 'handle_count', 'url_count']].to_csv(\"../data/interim/new_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf251a",
   "metadata": {},
   "source": [
    "# Generation of Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for generating translation and embeddings\n",
    "\n",
    "def detect_language(df):\n",
    "    \"\"\"Predict language used in `description` using FastText model.\"\"\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"Remove emoji and links from text to make prediction faster\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Emoji & link cleaner\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"\n",
    "                u\"\\U0001F300-\\U0001F5FF\"\n",
    "                u\"\\U0001F680-\\U0001F6FF\"\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        text = re.sub(r\"http\\S+|www\\S+|@\\S+\", \"\", text)\n",
    "        text = emoji_pattern.sub(r\"\", text)\n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "    # load fastText model\n",
    "    model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "    model = fasttext.load_model(model_path)\n",
    "    \n",
    "    def detect_text_language(text):\n",
    "        text = clean_text(text)\n",
    "        if not text:\n",
    "            return \"unknown\"\n",
    "        labels, probs = model.predict(text)\n",
    "        probs = np.asarray(probs)  # fix for NumPy 2.x\n",
    "        return labels[0].replace(\"__label__\", \"\")\n",
    "\n",
    "    df[\"description_language\"] = df[\"description\"].apply(detect_text_language)\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "def translate_language(\n",
    "        df,\n",
    "        translation_model_map={\n",
    "        \"yue_Hant\": pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\"),\n",
    "        \"kor_Hang\": pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ko-en\"),\n",
    "        \"spa_Latn\": pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "    },\n",
    "        chunk_size=1000):\n",
    "    \n",
    "        df[\"description_en\"] = \"\"\n",
    "\n",
    "        for start in range(0, len(df), chunk_size):\n",
    "            end = min(start + chunk_size, len(df))\n",
    "            chunk = df.iloc[start:end]\n",
    "\n",
    "            for i, row in chunk.iterrows():\n",
    "                lang = row[\"description_language\"]\n",
    "                text = row[\"description\"]\n",
    "\n",
    "                if lang == \"eng_Latn\":\n",
    "                    translated = text\n",
    "                elif lang == \"unknown\" or lang not in translation_model_map:\n",
    "                    translated = \"\"\n",
    "                else:\n",
    "                    translator = translation_model_map[lang]\n",
    "                    translated = translator(text)\n",
    "\n",
    "                df.at[i, \"description_en\"] = translated\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    \"\"\"\n",
    "    Remove translated pattern from descriptions that were translated to english.\n",
    "    Normalize  by replacing handles and URLs with placeholders, lowercase and lemmatize.\n",
    "    \"\"\"\n",
    "    translated_pattern = r\"'translation_text': '([^']*)'\"\n",
    "    match = re.search(translated_pattern, text)\n",
    "    if match:\n",
    "        text = match[1]\n",
    "\n",
    "    # replace URLs with placeholder\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "\n",
    "    # remove newlines, tabs, and extra spaces\n",
    "    text = re.sub(r\"[\\n\\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    filtered = [word for word in text.split() if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in filtered]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "def embed_descriptions(description_df, hf_api):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', token=hf_api)\n",
    "    embeddings_lst = []\n",
    "\n",
    "    for i, row in description_df.iterrows():\n",
    "        if row[\"description_en\"]:\n",
    "            embeddings = model.encode(row[\"description_en\"])\n",
    "        else:\n",
    "            embeddings = np.zeros(384, dtype=float)  # default dim=384\n",
    "        embeddings_lst.append(embeddings)\n",
    "\n",
    "        if not i % 1000:\n",
    "            print(f\"progress: {i}\")\n",
    "\n",
    "    description_df[\"description_en_embeddings\"] = embeddings_lst\n",
    "    \n",
    "    return description_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = detect_language(df)\n",
    "df = translate_language(df)\n",
    "df = embed_descriptions(df, os.getenv(\"HF_API_KEY\"))\n",
    "\n",
    "# # preprocess translated user descriptions and impute missing values with empty string\n",
    "# df[\"description_en\"] = df[\"description_en\"].fillna(\"\")\n",
    "# df[\"description_en\"] = df[\"description_en\"].apply(custom_preprocessor)\n",
    "\n",
    "# save to csv\n",
    "df[['id', 'description_language','description_en', 'description_en_embeddings']].to_csv(\"../data/interim/translated_embeddings.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
